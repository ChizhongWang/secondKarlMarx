{
  "os": "Linux-5.15.0-105-generic-x86_64-with-glibc2.35",
  "python": "CPython 3.11.8",
  "startedAt": "2025-04-28T15:14:28.629197Z",
  "args": [
    "qwen_lora_sft.yaml"
  ],
  "program": "/home/featurize/secondKarlMarx/LLaMA-Factory/src/llamafactory/launcher.py",
  "codePath": "LLaMA-Factory/src/llamafactory/launcher.py",
  "git": {
    "remote": "https://github.com/ChizhongWang/secondKarlMarx.git",
    "commit": "ded4f1c6eaeee2cb6166299e19ca87f47be7bfb4"
  },
  "root": "/home/featurize/secondKarlMarx",
  "host": "featurize",
  "executable": "/environment/miniconda3/bin/python",
  "codePathLocal": "LLaMA-Factory/src/llamafactory/launcher.py",
  "cpu_count": 16,
  "cpu_count_logical": 16,
  "gpu": "NVIDIA GeForce RTX 4090",
  "gpu_count": 1,
  "disk": {
    "/": {
      "total": "728249831424",
      "used": "45555949568"
    }
  },
  "memory": {
    "total": "58965082112"
  },
  "cpu": {
    "count": 16,
    "countLogical": 16
  },
  "gpu_nvidia": [
    {
      "name": "NVIDIA GeForce RTX 4090",
      "memoryTotal": "25757220864",
      "cudaCores": 16384,
      "architecture": "Ada"
    }
  ],
  "cudaVersion": "12.6"
}